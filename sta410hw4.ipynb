{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e179f5",
   "metadata": {},
   "source": [
    "# STA410 Programming Portfolio Assignment 4 (20 points)\n",
    "\n",
    "Welcome.\n",
    "\n",
    "## Rules\n",
    "\n",
    "\n",
    "0. Point awards for assigning the correct values into variables are indicated along with each required variable assignment name.  \n",
    "\n",
    "\n",
    "1. You are welcome to adapt code you find available online into your notebook; however, if you do so you must provide a link to the utilized resource. \n",
    "\n",
    "    - If failure to cite such references is identified and confirmed, your score will be immediately reduced to 0.\n",
    "\n",
    "\n",
    "2. **Do not delete or replace cells**: this erases `cell ids` upon which automated scoring is based.\n",
    "\n",
    "    - ***If you are working in any environment other than*** [UofT JupyterLab](https://jupyter.utoronto.ca/hub/user-redirect/git-pull?repo=https://github.com/pointOfive/sta410hw4&branch=main&urlpath=/lab/tree/sta410hw4), [UofT JupyterHub](https://jupyter.utoronto.ca/hub/user-redirect/git-pull?repo=https://github.com/pointOfive/sta410hw4&branch=master), or [Google Colab](https://colab.research.google.com/github/pointOfive/sta410hw4/blob/main/sta410hw4.ipynb), that environment must meet the versioning requirements of\n",
    "    \n",
    "        - [notebook format >=4.5](https://github.com/jupyterlab/jupyterlab/issues/9729) \n",
    "        - jupyter [notebook](https://jupyter.org/install#jupyter-notebook) version [>=6.2](https://jupyter-notebook.readthedocs.io/en/stable/) for \"classic\" notebooks served by [jupyterhub](https://jupyterhub.readthedocs.io/en/stable/quickstart.html)\n",
    "        - [jupyterlab](https://jupyter.org/install) version [>=3.0.13](https://github.com/jupyterlab/jupyterlab/releases/tag/v3.0.13) for \"jupyterlab\" notebooks  \n",
    "\n",
    "      so that cell-ids do not randomize.  Otherwise `cell ids` will not be supported and you will not get any credit for your submitted homework. \n",
    "\n",
    "      > You may check if cell ids are present or changing at each save with `! grep '\"id\":' <path/to/notebook>.ipynb`\n",
    "\n",
    "    - ***You may add cells for scratch work*** but if required answers are not submitted through the provided cells where the answers are requested your answers will not be graded.\n",
    "    - ***If you accidentally delete a required cell*** try \"Edit > Undo Delete Cells\" in the notebook editor; otherwise, redownload the notebook (so it has the correct required cells ids) and repopulate it with your answers (assuming you don't overwrite them).\n",
    "    \n",
    "    \n",
    "3. **No cells may have any runtime errors** because this causes subsequent tests to fail and you will not get credit for tests which fail because of previous runtime errors.\n",
    "\n",
    "  > Run time errors include, e.g., unassigned variables, mismatched parentheses, any any code which does not work when the notebook cells are sequentially run, even if it was provided for you as part of the starter code.\n",
    "    \n",
    "    - The `try`-`except` block syntax catches runtime errors and transforms them into `exceptions` which will not cause subsequent tests to fail.    \n",
    "    \n",
    "    - **No jupyter shortcut commands** such as `! python script.py 10` or `%%timeit` may be included in the final submission as they will cause subsequent tests to fail.\n",
    "\n",
    "        > Comment out jupyter shortcut commands, e.g., `# ! python script.py 10` or `# %%timeit` in submitted notebooks.\n",
    "\n",
    "\n",
    "4. Specific code solutions submitted for these assignments must be created either individually or in the context of a paired effort.\n",
    "  \n",
    "  - Students may work individually.  \n",
    "    - Students choosing to work individually must work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity)  of \"honesty, trust, fairness, respect, responsibility and courage.\"\n",
    "  - Students may self-select pairs for each assignment.\n",
    "    - Paired students work together and may share work without restriction within their pair; but, must otherwise work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity) noted above.\n",
    "    - Paired students each separately submit their (common) work, including (agreeing) contribution of work statements for each problem.\n",
    "    \n",
    "    *Please seek homework partners in person or on the course discussion board on Quercus. Groups of three or more are not allowed; however, students are welcome to amicably seek new partners for each new assignment.* \n",
    "\n",
    "  ***Getting and sharing \"hints\" from other classmates is allowed; but, the eventual code creation work and submission must be your own individual or paired creation.***\n",
    "\n",
    "\n",
    "5. Unless a problem instructs differently, you may use any functions available from all library imports below; otherwise, you are expected to create your own Python functionality based on the Python stdlib (standard libary).\n",
    "\n",
    "    - Importing any libraries besides those specified below, those specifically suggested by a problem prompt, or the [standard python modules](https://docs.python.org/3/py-modindex.html) will cause a runtime error which will result in a loss of points.\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f578ec",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from PIL import Image\n",
    "import pymc3 as pm\n",
    "import requests, shutil\n",
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tfkl = tfk.layers\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbf1f7",
   "metadata": {},
   "source": [
    "# Problem 0 (required)\n",
    "\n",
    "Are you working with a partner to complete this assignment?  \n",
    "- If not, assign  the value of `None` into the variable `Partner`.\n",
    "- If so, assign the name of the person you worked with into the variable `Partner`.\n",
    "    - Format the name as `\"<First Name> <Last Name>\"` as a `str` type, e.g., \"Scott Schwartz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285a8a1",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required: only worth points when not completed, in which case, you'll lose points\n",
    "Partner = #None\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45a658",
   "metadata": {},
   "source": [
    "What was your contribution in completing the code for this assignments problems? Assign one of the following into each of the `Problem_X` variables below.\n",
    "\n",
    "- `\"I worked alone\"`\n",
    "- `\"I contributed more than my partner\"`\n",
    "- `\"My partner and I contributed equally\"`\n",
    "- `\"I contributed less than my partner\"`\n",
    "- `\"I did not contribute\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b4018",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required: only worth points when not completed, in which case, you'll lose points\n",
    "Problem_1 = #\"I worked alone\"\n",
    "Problem_2 = #\"I worked alone\"\n",
    "Problem_3 = #\"I worked alone\"\n",
    "Problem_4 = #\"I worked alone\"\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320568d",
   "metadata": {},
   "source": [
    "# Problem 1 (5 points)\n",
    "\n",
    "0. Go to https://docs.pymc.io/en/v3/\n",
    "1. Click the \"Quickstart\" buttom, which is the first thing you see on the page under \"Probabilistic Programming in Python\"\n",
    "2. Complete the tutorial by copying and running cells in your own nobebook envirionment\n",
    "    - feel free to skip redundant cells, e.g., \"cell 4\" completes `# Model definition` in \"cell 3\", so just run \"cell 4\".\n",
    "    - feel free to combine related cells, e.g., just run `model.basic_RVs, model.free_RVs, model.observed_RVs, model.logp({\"mu\": 0})` in a single cell rather than in 4 separate cells.\n",
    "    - feel free to skip running \"information cells\" like `help(pm.Normal)` or `dir(pm.distributions.mixture)`, though certainly these are extremely useful functionalities\n",
    "3. Answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b241a",
   "metadata": {},
   "source": [
    "#### p1q0. What `keyword` differentiates observed and unobserved randomvariables in `PyMC`? \n",
    "\n",
    "- (A) observed\n",
    "- (B) unobserved\n",
    "- (C) observation\n",
    "- (D) random_variable\n",
    "\n",
    "\n",
    "#### p1q1. What data types can be used to assign observed values to random variables in `PyMC`?\n",
    "\n",
    "- (A) lists\n",
    "- (B) lists and numpy.ndarray\n",
    "- (C) lists, np.ndarray, and theano data structures\n",
    "- (D) lists, np.ndarray, theano and pandas data structures\n",
    "\n",
    "\n",
    "#### p1q2. Where can you access math functions for deterministically transforming random variables in `PyMC`?\n",
    "\n",
    "- (A) pm\n",
    "- (B) math\n",
    "- (C) pm.math\n",
    "- (D) np.linalg\n",
    "\n",
    "#### p1q3. Where can you access random variable distributions, such as `Normal`, `Gamma`, `Uniform`, etc. in `PyMC`?\n",
    "\n",
    "- (A) pm\n",
    "- (B) math\n",
    "- (C) pm.math \n",
    "- (D) np.linalg\n",
    "\n",
    "#### p1q4. The tutorial states \"*In order to sample models more efficiently, PyMC3 automatically transforms bounded RVs to be unbounded.*\"  What do you suppose this means?  Note: HMC stands for Hamiltonian Monte Carlo.\n",
    "- (A) HMC is based on gradient based movement, so it's easier to not have to correct for \"out of bounds\"\n",
    "- (B) Discrete random variables need to be converted to continuous random variables for HMC to work well\n",
    "- (C) It's not about bounds on random variables, it's just that unbounded floating point is more accurate.\n",
    "- (D) It doesn't mean anything and is just an arbitrary choice that doesn't really matter computationally.\n",
    "\n",
    "#### p1q5. What is printed out when you run `model.free_RVs, model.deterministics` associated with `x = pm.Uniform(\"x\", lower=0, upper=1)` from \"cell 17\"?  Hint: it's not \"[[x_interval], [x]]\".\n",
    "\n",
    "- (A) ([x_interval__ : TransformedDistribution], [x : Uniform])\n",
    "- (B) ([x_interval__ ~ TransformedDistribution], [x ~ Uniform])\n",
    "- (C) [[x_interval__ ~ TransformedDistribution], [x ~ Uniform]]\n",
    "- (D) (x_interval__, x)\n",
    "\n",
    "#### p1q6. PyMC can provide the functionality that [TensorFlow Probability](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Bijector) calls \"bijectors\".  Which of the following is useful for using \"bijectors\" in PyMC?\n",
    "\n",
    "- (A) Knowing the `backward`, `forward`, and `jacobian_det` components of a change of varaibles transformation\n",
    "- (B) The `tr.ElemwiseTransform` class from `import pymc3.distributions.transforms as tr` for defining custom transformations\n",
    "- (C) Built-in \"bijectors\" from `import pymc3.distributions.transforms as tr`, such as the `tr.LogOdds` transform for `Uniform` RVs.\n",
    "- (D) All of the above\n",
    "\n",
    "\n",
    "#### p1q7.  What happens to the left plot in \"cell 24\" when `tr.Chain([Logodd, Order])` is replaced by `tr.Chain([Logodd])`?\n",
    "\n",
    "*Note: the inverse log odds (i.e., the inverse logit) and the $x_1 \\leftrightarrow x_2$ swap are applied to the data on the left to transform it into the data on the right; or, the inverse of these (i.e., the logit and the opposite but effectively the same $x_2 \\leftrightarrow x_1$ swap is applied to the data on the right to transform it into the data on the left.*\n",
    "\n",
    "- (A) The point cloud becomes spherical [A]\n",
    "- (B) The point cloud becomes uniform over a square shape\n",
    "- (C) The point cloud becomes uniform over a diamond (rotated square) shape\n",
    "- (D) The plot does not work because `[Logodd]` is not a list\n",
    "\n",
    "#### p1q8.  What happens to the left plot in \"cell 24\" when `tr.Chain([Logodd, Order])` is replaced by `tr.Chain([Logodd])`?\n",
    "\n",
    "- (A) The point cloud becomes spherical\n",
    "- (B) The point cloud becomes uniform over a square shape\n",
    "- (C) The point cloud becomes uniform over a diamond (rotated square) shape\n",
    "- (D) The plot does not work because `[Logodd]` is not a list\n",
    "\n",
    "#### p1q9. Which of the following is the correct way to specify a 10 dimensional random variable?\n",
    "\n",
    "- (A) `x = [pm.Normal(f\"x_{i}\", mu=0, sigma=1) for i in range(10)]`\n",
    "- (B) `x = pm.Normal(\"x\", mu=0, sigma=1, shape=10)`\n",
    "- (C) Either of the above are the correct specificiation\n",
    "- (D) Neither of the above are the correct specificiation\n",
    "\n",
    "#### p1q10. What is the output of the following code which uses the `.get_test_value()` method based on the `testval` argument?\n",
    "\n",
    "```\n",
    "np.random.seed(10)\n",
    "with pm.Model():\n",
    "    x = pm.Normal(\"x\", mu=0, sigma=1, shape=5, testval=np.random.randn(5))\n",
    "    print((x.dot(x.T)).get_test_value())\n",
    "```    \n",
    "\n",
    "- (A) 5.192\n",
    "- (B) 5.059\n",
    "- (C) 10.417\n",
    "- (D) None of the above\n",
    "\n",
    "\n",
    "#### p1q11. What is recommended in the case of complex models that are hard for the \"No U-Turn\" sampler (NUTS)?\n",
    "\n",
    "- (A) Use `NUTS` anyway but improve the initialization or reparameterize the model\n",
    "- (B) Use `Metropolis`\n",
    "- (C) Use `Slice`\n",
    "- (D) Use `MCMC` or `HMC`\n",
    "- (E) Stop being Bayesian because asymptotic distributions for complex models are easy to derive\n",
    "\n",
    "\n",
    "#### p1q12. Which of the following is a not parameter of the `pm.sample` function? \n",
    "\n",
    "- (A) chains\n",
    "- (B) observed\n",
    "- (C) step\n",
    "- (D) tune\n",
    "\n",
    "\n",
    "#### p1q13. Which of the following is not a method available for the `step` parameter in PyMC? \n",
    "\n",
    "- (A) pm.BinaryGibbsMetropolis()\n",
    "- (B) pm.DEMetropolisZ()\n",
    "- (C) pm.HMC()\n",
    "- (D) pm.LaplaceProposal()\n",
    "\n",
    "#### p1q14. What is the purpose of the \"forest plot\"\n",
    "\n",
    "- (A) To determine the number of independent Markov chains that should be run\n",
    "- (B) To visualize and interpret predictions from random forest ensemble models\n",
    "- (C) To characterize the degree of autocorrelation present in Markov chains\n",
    "- (D) To see if samples disagree across chains, indicating lack of convergence\n",
    "\n",
    "#### p1q15. Run `np.random.seed(15)`, then \"cell 37\" run, and then `az.plot_posterior(idata);` in that sequence. How does the plot differ from the output of the tutorial?\n",
    "\n",
    "- (A) It does not differ in any notable or substantial way\n",
    "- (B) The KDE is more \"jagged\" and less \"smooth\"\n",
    "- (C) The credible intervals don't overlap\n",
    "- (D) There is a difference but it's just \"run to run variability\"\n",
    "\n",
    "#### p1q16. Something funny may be going on in the context of the previous question. What is it?\n",
    "\n",
    "- (A) Nothing funny is going on it's just \"run to run variability\"\n",
    "- (B) The tutorial may not be actually using `step=[step1, step2]`\n",
    "- (C) The posteriors being plotted don't correspond to the model in \"cell 37\"\n",
    "- (D) The tutorial R-hat values in `az.summary(idata)` and `az.plot_forest(idata, r_hat=True);` don't match \n",
    "\n",
    "#### p1q17. Run `np.random.seed(15)`, then \"cell 37\" run, and then `az.plot_energy(idata);` in that sequence as in \"p1q15\". What does the engergy plot look like?\n",
    "\n",
    "- (A) The marginal energy and the energy transition are no longer unimodal\n",
    "- (B) The marginal energy and the energy transition overlap as in the tutorial\n",
    "- (C) The marginal energy and the energy transition do not overlap as well as in the tutorial\n",
    "- (D) There is no plot due to `AttributeError: 'Dataset' object has no attribute 'energy'`\n",
    "\n",
    "\n",
    "#### p1q18. What benefit does the \"energy\" characterization available from NUTS provide?\n",
    "\n",
    "*Notes: The \"No U-Turn\" sampler (NUTS) is a specific Hamiltonian Monte Carlo (HMC) algorithm implementation.\n",
    "The \"energy\" referred to in PyMC is from the Hamiltonian dynamics (discussed in the \"Symplectic Integration\" section from the first part of the class notes).\n",
    "HMC methods, e.g., NUTS, use Hamiltonian dynamics to move along probability contours, but proposals must also transition between different contour levels as well.*\n",
    "\n",
    "- (A) It can be used to create credible intervals of an overall global parameter\n",
    "- (B) It's the best way assess convergence when the parameter dimension very small \n",
    "- (C) It can be used to assess convergence when the parameter dimension very large\n",
    "- (D) It's not useful because it's the log joint density which includes the auxiliary parameters\n",
    "\n",
    "\n",
    "#### p1q19.  What does `az.plot_energy(short_trace);` look like for the following code?\n",
    "\n",
    "```\n",
    "# https://docs.pymc.io/en/v3/pymc-examples/examples/diagnostics_and_criticism/Diagnosing_biased_Inference_with_Divergences.html\n",
    "# Data of the Eight Schools Model\n",
    "J = 8\n",
    "y = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\n",
    "sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])\n",
    "# tau = 25.\n",
    "SEED = [20100420, 20134234]\n",
    "with pm.Model() as Centered_eight:\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=5)\n",
    "    tau = pm.HalfCauchy(\"tau\", beta=5)\n",
    "    theta = pm.Normal(\"theta\", mu=mu, sigma=tau, shape=J)\n",
    "    obs = pm.Normal(\"obs\", mu=theta, sigma=sigma, observed=y)\n",
    "\n",
    "with Centered_eight:\n",
    "    short_trace = pm.sample(600, chains=2, random_seed=SEED)\n",
    "\n",
    "az.plot_energy(short_trace);  \n",
    "```\n",
    "\n",
    "- (A) It looks like the `az.plot_energy(short_trace);` plot in the tutorial\n",
    "- (B) The energy transition distribution is narrower than the marginal energy distribution\n",
    "- (C) The energy transition distribution is wider than the marginal energy distribution\n",
    "- (D) There is no plot due to `AttributeError: 'Dataset' object has no attribute 'energy'`\n",
    "\n",
    "\n",
    "#### p1q20. What is the difference between `ADVI` and `fullrank_ADVI`\n",
    "\n",
    "- (A) One is gradient based an the other is not.\n",
    "- (B) The rank of the covariance matrix which they specify.\n",
    "- (C) One is based on normal distributions while the other is not\n",
    "- (D) Independent normal specifications versus general multivariate specifications\n",
    "\n",
    "\n",
    "#### p1q21. What model fitting techniques available in PyMC are demonstrated in the tutorial?\n",
    "\n",
    "- (A) MCMC/HMC\n",
    "- (B) MCMC/HMC, Variational Inference\n",
    "- (C) MCMC/HMC, Variational Inference, Particle filtering\n",
    "- (D) MCMC/HMC, Variational Inference, Particle filtering, Maximum Likelihood Estimation\n",
    "\n",
    "#### p1q22. After running \"cell 48\" and \"cell 49\", next run `len(set(approx.sample(50000)['x']))` and `az.plot_dist(trace['x']);` and comment on the result.\n",
    "\n",
    "- (A) The KDE of `trace['x'])` remains the same as it is in the tutorial\n",
    "- (B) With just 200 unique \"particles\" a smooth KDE is not an accurate representation of the sample\n",
    "- (C) The \"particle filtering\" method appears to be broken or is just not a useful method\n",
    "- (D) There is no plot due to `AttributeError: 'Dataset' object has no attribute 'energy'`\n",
    "\n",
    "#### p1q23. What are the \"posterior predictive\" lines?\n",
    "\n",
    "*Note: KDE stands for kernel density estimate produced by `az.plot_dist`.*\n",
    "\n",
    "- (A) KDEs of samples of data from models which each correspond to a draw from the posterior distribution\n",
    "- (B) The KDE of the original sample data used to create the posterior distribution\n",
    "- (C) The average of the \"posterior predictive\" KDEs \n",
    "- (D) The actual mean from which the data was produced\n",
    "\n",
    "#### p1q24. Run \"cell 53\", \"cell 54\", and \"cell 55\" in order with the code adjustments given below.  What is the sum of squared errors for the posterior predictive mean of the hold out data exactly as given in \"cell 55\"?\n",
    "\n",
    "Run `np.random.seed(24)` before the code in \"cell 53\"; and, using the same `SEED` from \"p1q19\", add `random_seed=SEED` into the `pm.sample` function of \"cell 54\") and `pm.sample_posterior_predictive` function of \"cell 55\".\n",
    "\n",
    "- (A) 0.96950315\n",
    "- (B) 1.17084275\n",
    "- (C) 0.0\n",
    "- (D) None of the above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c61ef",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# p1q0-q24: 1/5 point each [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" based on the choices above]\n",
    "p1q0 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q1 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q2 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q3 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q4 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q5 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q6 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q7 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q8 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q9 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q10 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q11 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q12 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q13 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q14 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q15 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q16 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q17 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q18 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q19 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q20 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q21 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q22 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q23 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q24 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e4c65",
   "metadata": {},
   "source": [
    "# Problem 2 (5 points)\n",
    "\n",
    "Update the `AdaptiveSqueezedRejectionSample` method (only, i.e., not other code should be changed) \n",
    "of the `AdaptiveSqueezedRejectionSampling` class below\n",
    "so the code executes the ***(derivative-based) adaptive (squeezed) rejection sampling*** algorithm:\n",
    "\n",
    "> 0. Given upper $uh_k(x)$ and lower $lh_k(x)$ hulls of continuous piecewise linear functions which respectively \n",
    ">    - are constructed by the tangets and secants of $k$ points\n",
    ">    - bound the convex log likelihood of a target distribution $f(x)$ from above and below  \n",
    ">\n",
    ">\n",
    "> 1. Sample $x^*$ from the exponentiated upper hull (proposal distribribution) which is an unnormalized continuous density of piecewise truncated scaled exponential distributions connected end to end\n",
    ">\n",
    ">    1. Sample a unform random variable $u$ \n",
    ">    2. Accept the proposal $x^*$ immediately if $u < e^{lh_k(x^*)-uh_k(x^*)}$, i.e., with probability $e^{lh_k(x^*)-uh_k(x^*)}$ \n",
    ">    3. If $x^*$ is not accepted in B, accept proposal if $u < f(x^*)/e^{uh_k(x^*)}$\n",
    ">       1. Accepting $x^*$ at stage C will happen with probability $\\frac{f(x^*)-lh_k(x^*)}{e^{uh_k(x^*)}}$\n",
    ">       2. If $x^*$ is accepted at stage C, add $x^*$ to the $k$ points defining $uh_k(x)$ and $uh_k(x)$ and update the upper and lower hulls to $uh_{k+1}(x)$ and $lh_{k+1}(x)$ \n",
    "> 2. If $x^*$ was not accepted in B or C above, discard $x^*$ \n",
    ">\n",
    ">\n",
    "> 3. Return to step 1 as desired\n",
    "\n",
    "*This problem is inspired by sections 6.2.3.2 and 6.2.3.1 **Adaptive Rejection Sampling** and **Squeezed Rejection Sampling** on pages 158-163 in Chapter 6.2.3 **Exact Simulation: Rejection Sampling** of the Givens and Hoeting **Computational Statistics** textbook which introduce the **Adaptive Rejection Sampling** methodology that formed the basis of the first-generation universal probabilistic programming tool **WinBugs**, which (for many models) allowed users to automatically draw samples from the posterior of a Bayesian analysis based just on an initial probability model (likelihood and a prior) specification and not requiring the user to perform any of their own derivations of the posterior distribution.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b68268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class AdaptiveSqueezedRejectionSampling(object):\n",
    "\n",
    "    def __init__(self, target, logpdf, scoref, xk):\n",
    "        self.target, self.logpdf, self.scoref = target, logpdf, scoref\n",
    "        self.Adapt(list(xk))\n",
    "\n",
    "    def UpperHull(self, x):\n",
    "        @np.vectorize\n",
    "        def _UpperHull(x):\n",
    "            i = np.sum(x > self.zk)\n",
    "            return self.logpdf(self.xk[i]) + (x-self.xk[i])*self.scoref(self.xk[i])\n",
    "        return _UpperHull(x)\n",
    "    \n",
    "    def LowerHull(self, x):\n",
    "        @np.vectorize\n",
    "        def _LowerHull(x):\n",
    "            if x<self.xk[0] or x>self.xk[-1]:\n",
    "                return -np.Inf\n",
    "            else:\n",
    "                i = np.sum(x > self.xk)-1\n",
    "                return ((self.xk[i+1]-x)*self.logpdf(self.xk[i]) + \n",
    "                        (x-self.xk[i])*self.logpdf(self.xk[i+1]))/(self.xk[i+1]-self.xk[i])\n",
    "        return _LowerHull(x)\n",
    "\n",
    "    def ExponentiatedUpperHullIntervalAreas(self):\n",
    "        p = 0*self.xk\n",
    "        for i in range(len(self.xk)):\n",
    "            if i == 0:\n",
    "                a, b = -np.Inf, self.zk[i]\n",
    "            elif i == (len(self.xk)-1):\n",
    "                a, b = self.zk[i-1], np.Inf\n",
    "            else:\n",
    "                a, b = self.zk[i-1], self.zk[i]\n",
    "            p[i] = self.target(self.xk[i])*np.exp(-self.xk[i]*self.scoref(self.xk[i]))\n",
    "            p[i] = p[i]*( np.exp(self.scoref(self.xk[i])*b) -\n",
    "                          np.exp(self.scoref(self.xk[i])*a) )/self.scoref(self.xk[i])\n",
    "        return p\n",
    "\n",
    "    def SampleUpperHullIntervals(self, n):\n",
    "        p = self.ExponentiatedUpperHullIntervalAreas()\n",
    "        return np.argmax(stats.multinomial.rvs(1, p/p.sum(), size=n), axis=1)\n",
    "\n",
    "    def SampleFromIntervals(self, i):\n",
    "        @np.vectorize\n",
    "        def _SampleFromInterval(i):\n",
    "            if i == 0:\n",
    "                return self.zk[i] - stats.expon(scale=1/self.scoref(self.xk[i])).rvs(size=1)[0]\n",
    "            elif i == len(self.xk)-1:\n",
    "                return self.zk[i-1] + stats.expon(scale=-1/self.scoref(self.xk[i])).rvs(size=1)[0]\n",
    "            elif scoref(self.xk[i]) < 0:\n",
    "                return self.zk[i-1] + \\\n",
    "                       stats.truncexpon(scale=-1/self.scoref(self.xk[i]), \n",
    "                                        b=-(self.zk[i]-self.zk[i-1])*self.scoref(self.xk[i])).rvs(size=1)[0]\n",
    "            else:\n",
    "                return self.zk[i] - \\\n",
    "                       stats.truncexpon(scale=1/self.scoref(self.xk[i]), \n",
    "                                        b=(self.zk[i]-self.zk[i-1])*scoref(self.xk[i])).rvs(size=1)[0]\n",
    "        return _SampleFromInterval(i)\n",
    "\n",
    "    def SampleFromExponentiatedUpperHull(self, n):\n",
    "        return self.SampleFromIntervals(self.SampleUpperHullIntervals(n))\n",
    "\n",
    "    def AdaptiveSqueezedRejectionSample(self):\n",
    "        x = stats.uniform.rvs(size=1) # wrong\n",
    "        u = stats.uniform.rvs(size=1)\n",
    "        if u < x: # wrong\n",
    "            return x\n",
    "        elif u < x**0.5: # wrong\n",
    "            # self.Adapt(...) missing\n",
    "            return x\n",
    "        else:\n",
    "            return self.AdaptiveSqueezedRejectionSample()\n",
    "        \n",
    "    def Adapt(self, xk):\n",
    "        self.xk = np.sort(xk)\n",
    "        self.zk = (self.logpdf(self.xk[1:]) - self.logpdf(self.xk[:-1]) - \n",
    "                   self.xk[1:]*self.scoref(self.xk[1:]) + self.xk[:-1]*self.scoref(self.xk[:-1])) \n",
    "        self.zk = self.zk / (self.scoref(self.xk[:-1])-self.scoref(self.xk[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "mu, sigma=0,1\n",
    "target = lambda x: stats.norm(loc=mu,scale=sigma).pdf(x)\n",
    "logpdf = lambda x: stats.norm(loc=mu,scale=sigma).logpdf(x)\n",
    "scoref = lambda x: -(x-0)/sigma**2 \n",
    "xk = np.array([-3., .1, 3.])\n",
    "\n",
    "ASRS = AdaptiveSqueezedRejectionSampling(target, logpdf, scoref, xk)\n",
    "\n",
    "fig,ax = plt.subplots(3, 2, figsize=(10,10))\n",
    "support = np.linspace(-4,4,100)\n",
    "\n",
    "ax[0,0].plot(support, ASRS.UpperHull(support))\n",
    "ax[0,0].plot(support, ASRS.logpdf(support))\n",
    "ax[0,0].plot(support, ASRS.LowerHull(support))\n",
    "ax[0,0].set_title(\"Initial Upper and Lower Hulls\")\n",
    "\n",
    "ax[0,1].fill_between(support, 0, np.exp(ASRS.UpperHull(support)))\n",
    "ax[0,1].fill_between(support, 0, ASRS.target(support))\n",
    "ax[0,1].fill_between(support, 0, np.exp(ASRS.LowerHull(support)))\n",
    "ax[0,1].set_title(\"Exponentiated Upper and Lower Hulls\\nand Target Distribution\")\n",
    "\n",
    "ax[1,0].hist(ASRS.SampleFromExponentiatedUpperHull(10000), bins=100, density=True)\n",
    "c = ASRS.ExponentiatedUpperHullIntervalAreas().sum()\n",
    "ax[1,0].plot(support, np.exp(ASRS.UpperHull(support))/c)\n",
    "ax[1,0].set_title(\"Initial Sampling from Exponentiated Upper Hull\")\n",
    "\n",
    "np.random.seed(500)\n",
    "n = 500\n",
    "ax[1,1].hist([ASRS.AdaptiveSqueezedRejectionSample() for i in range(n)], density=True, bins=20)\n",
    "ax[1,1].plot(support, ASRS.target(support))\n",
    "ax[1,1].set_title(\"Adaptive Squeezed Rejection Sampling\")\n",
    "\n",
    "ax[2,0].hist(stats.norm.rvs(size=n), density=True, bins=20)\n",
    "ax[2,0].plot(support, ASRS.target(support))\n",
    "ax[2,0].set_title(\"Exact Sampling from Target Distribution\")\n",
    "\n",
    "ax[2,1].plot(support, ASRS.UpperHull(support))\n",
    "ax[2,1].plot(support, ASRS.logpdf(support))\n",
    "ax[2,1].plot(support, ASRS.LowerHull(support))\n",
    "ax[2,1].set_title(\"Adapted Upper and Lower Hulls\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b924d585",
   "metadata": {},
   "source": [
    "## Hints:\n",
    "\n",
    "- The questions below are designed to help you complete the necessary code.\n",
    "    - Only update the `AdaptiveSqueezedRejectionSample` method.  \n",
    "    - Nothing else in the `AdaptiveSqueezedRejectionSampling` class and related code should be changed.\n",
    "    \n",
    "    \n",
    "- Why doesn't, e.g., `ASRS.UpperHull(support)` appear to match it's defined fucntion signature `def UpperHull(self, x):`?\n",
    "    - Because `self` automatically assumes the value `ASRS` when the `UpperHull` method is called from the `ASRS` object.\n",
    "\n",
    "\n",
    "- When in a class object, the object may call it's own methods and attributes as, e.g., \n",
    "    - `self.target(self.xk[i])`\n",
    "    - `self.SampleFromIntervals(self.SampleUpperHullIntervals(n))`\n",
    "    - `self.AdaptiveSqueezedRejectionSample()`    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abfdcf",
   "metadata": {},
   "source": [
    "### Problem 2 question 0-11 (2 points, 1/6 point each)\n",
    "\n",
    "0. What argument(s) needs to be passed to the `Adapt` method when called as a method?\n",
    "\n",
    "- (A) `self`\n",
    "- (B) `xk`\n",
    "- (C) `self` and `xk`\n",
    "- (D) None of the above\n",
    "\n",
    "1. The `k` on the `xk` parameter of the `Adapt` method refers to what?\n",
    "\n",
    "- (A) The $k^th$ elment of `x`, i.e., `k` refers to an index\n",
    "- (B) The number of samples created thus far by the A(S)RS algorithm\n",
    "- (C) The number of points used to construct the upper and lower hulls\n",
    "- (D) It doesn't refer to anything -- it's just an arbitrary parameter name\n",
    "\n",
    "2. What input should be given to the `Adapt` method?\n",
    "\n",
    "- (A) `int`\n",
    "- (B) `list`\n",
    "- (C) `str`\n",
    "- (D) `lambda x: ...`\n",
    "\n",
    "3. There are three possible outcomes in the `AdaptiveSqueezedRejectionSample` method. The last one in the `else` clause caputes which of the following cases?\n",
    "\n",
    "- (A) The proposal `x` was accepted with probability equal to the ratio of the upper and lower hulls $e^{lh_k(x^*)-uh_k(x^*)}$\n",
    "- (B) The proposal `x` was accepted with probability $\\frac{f(x^*)-e^{lh_k(x^*)}}{e^{uh_k(x^*)}}$\n",
    "- (C) The proposal `x` was not accepted with probability $\\frac{e^{uh_k(x^*)} - f(x^*)}{e^{uh_k(x^*)}}$\n",
    "- (D) None of the above\n",
    "\n",
    "4. There are three possible outcomes in the `AdaptiveSqueezedRejectionSample` method. The second one in the `elif` clause caputes which of the following cases where the `self.Adapt` method needs to be called?\n",
    "\n",
    "- (A) The proposal `x` was accepted with probability equal to the ratio of the upper and lower hulls $e^{lh_k(x^*)-uh_k(x^*)}$\n",
    "- (B) The proposal `x` was accepted with probability $\\frac{f(x^*)-e^{lh_k(x^*)}}{e^{uh_k(x^*)}}$\n",
    "- (C) The proposal `x` was not accepted with probability $\\frac{e^{uh_k(x^*)} - f(x^*)}{e^{uh_k(x^*)}}$\n",
    "- (D) None of the above\n",
    "\n",
    "5. There are three possible outcomes in the `AdaptiveSqueezedRejectionSample` method. The first one in the `if` clause caputes which of the following cases?\n",
    "\n",
    "- (A) The proposal `x` was accepted with probability equal to the ratio of the upper and lower hulls $e^{lh_k(x^*)-uh_k(x^*)}$\n",
    "- (B) The proposal `x` was accepted with probability $\\frac{f(x^*)-e^{lh_k(x^*)}}{e^{uh_k(x^*)}}$\n",
    "- (C) The proposal `x` was not accepted with probability $\\frac{e^{uh_k(x^*)} - f(x^*)}{e^{uh_k(x^*)}}$\n",
    "- (D) None of the above\n",
    "\n",
    "6. Which of the following funtions is $uh_k$? \n",
    "\n",
    "- (A) `self.LowerHull`\n",
    "- (B) `self.UpperHull`\n",
    "- (C) `self.target`\n",
    "- (D) `self.scoref`\n",
    "\n",
    "7. Which of the following funtions is $lh_k$? \n",
    "\n",
    "- (A) `self.LowerHull`\n",
    "- (B) `self.UpperHull`\n",
    "- (C) `self.target`\n",
    "- (D) `self.scoref`\n",
    "\n",
    "8. Which of the following funtions is $f$? \n",
    "\n",
    "- (A) `self.LowerHull`\n",
    "- (B) `self.UpperHull`\n",
    "- (C) `self.target`\n",
    "- (D) `self.scoref`\n",
    "\n",
    "9. What does `x = self.SampleFromIntervals(self.SampleUpperHullIntervals(1))[0]` do?\n",
    "\n",
    "- (A) It draws a piecewise segment of $uh_k$ proportionally to the areas of the segments which comprise $uh_k$\n",
    "- (B) It samples from the (exponential or truncated exponential) distribution which the selected piecewise segment is\n",
    "- (C) It draws sample proportionally from $uh_k$ so that it functions as a probability density function proposal distribution\n",
    "- (D) All of the above\n",
    "\n",
    "10. Why must the density be log concave?\n",
    "- (A) This is explicitly referencing the fact that all densities are either log concave or log convex\n",
    "- (B) So that the log likelihood can be computed efficiently and can be optimized through maximization\n",
    "- (C) So that the tangents of upper hull and the secants of the lower hull create an envolope around $f$\n",
    "- (D) It does not need to be log concave for the Adaptive Squeezed Rejection Sampling algorithm to work\n",
    "\n",
    "11. How many points are used to construct the upper and lower hulls once the `AdaptiveSqueezedRejectionSample` method is fixed and the code above is run?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75e591",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# p2q0-q10: 1/6 point each [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" based on the choices above]\n",
    "p2q0 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q1 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q2 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q3 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q4 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q5 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q6 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q7 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q8 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q9 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q10 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "# p2q11: [format: `int`]\n",
    "p2q11 = #\n",
    "# This cell will produce a runtime error until the `p1q11` variable is assigned a value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ea380",
   "metadata": {},
   "source": [
    "### Problem 2 question 12  (3 points)\n",
    "\n",
    "Your code will be checked for accuracy.\n",
    "- No variable assignments are required for this question but your `AdaptiveSqueezedRejectionSampling` class must work when used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4edc2e",
   "metadata": {},
   "source": [
    "# Problem 3 (5 points)\n",
    "\n",
    "A simple version of a ***Normal-Normal Hidden Markov Model (HMM)*** for $t=1,\\cdots,T$ \n",
    "and a ***sequential importance sampling proposal*** for this ***HMM*** using the [slash distribution](https://en.wikipedia.org/wiki/Slash_distribution) are\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "Y_t \\sim {} & N(X_t, \\sigma^2) \\quad\\;  \\text{(observed)} &&& \\tilde X_t = {} & \\tilde X_{t-1} + Z_t/U_t \\quad \\text{(slash distribution proposal)}\\\\\n",
    "X_t \\sim {} & N(X_{t-1}, 1) \\quad \\text{(unobserved)} &&& Z_t \\sim {} & N(0, 1)\\\\\n",
    "X_0 \\equiv {} & 0 \\equiv \\tilde X_0 &&&U_t \\sim {} & U(0, 1)\n",
    "\\end{align*}\n",
    "\n",
    "The ***sequential importance weight*** for the $j^{th}$ ***importance sampling proposal*** random variable (sequence) $\\tilde X_{1:T} \\equiv (\\tilde X_1, \\cdots, \\tilde X_t, \\cdots \\tilde X_T)$, where $j$ is not included in the notation for brevity, is\n",
    "\n",
    "\\begin{align*}\n",
    "\\require{cancel}\n",
    "   W_1^* = {} & \\frac{p_{X_1|Y_1}(\\tilde x_1|Y_1=y_1)}{\\tilde p_{\\tilde X_1}(\\tilde x_1)} \\\\\n",
    "   = {} &\\frac{p_{Y_1 | X_1}(y_1 | \\tilde x_1) p_{X_1}(\\tilde x_1) \\overset{\\text{normalization}}{\\cancel{/ p_{Y_1}(y_1)}}}{\\tilde p_{\\tilde X_1}(\\tilde x_1) } \\quad \\text{where the normalized importance weight of the $j^{th}$ sample proposals is } W_{1j} = \\frac{W_{1j}^*}{\\sum_j W_{1j}^*}\\\\\n",
    "    W_t^* = {} &  \\frac{p_{X_{1:t},Y_{1:t}}(\\tilde x_{1:t}, y_{1:t})/p_{Y_{1:t}}(y_{1:t})}{\\tilde p_{\\tilde X_{1:t}}(\\tilde x_{1:t})} \\\\\n",
    "   = {} & \\underbrace{\\left(\\frac{p_{X_{1:(t-1)},Y_{1:(t-1)}}(\\tilde x_{1:(t-1)}, y_{1:(t-1)})}{ \\tilde p_{\\tilde X_{1:(t-1)}}(\\tilde x_{1:(t-1)})  }\\right)}_{W^*_{t-1}} \\frac{p_{X_t|X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})p_{Y_t|X_{t}}( y_t | \\tilde x_t)}{ \\tilde p_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1}) } \\frac{1}{\\underset{\\text{normalization}}{\\cancel{p_{Y_{1:t}}(y_{1:t})}}} \\quad \\text{again normalized over $j$ as } W_{jt} = \\frac{W_{jt}^*}{\\sum_j W_{jt}^*}\\\\\n",
    "   & {} \\text{with $\\tilde p_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1}) \\not =  p_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})$ not cancelling though for the specification $\\tilde X \\sim N(\\tilde X_{t-1},1)$ they would.}\n",
    "\\end{align*}\n",
    "\n",
    "Complete the `calculate_SIS_weights` function so that the `SIS_wResampling` function and the code below executes ***sequential importance sampling with resampling*** (also called [***sequential importance resampling***](https://en.wikipedia.org/wiki/Particle_filter#Sequential_Importance_Resampling_(SIR)))\n",
    "for this ***HMM*** and ***slash proposal distribution*** specification.  \n",
    "\n",
    "*This problem is inspired by Chapter 6.3.2 **Sequential Monte Carlo** and particularly draws upon sections 6.3.2.5 **Particle Filters** on page 179, 6.3.2.4 **Sequential Importance Sampling for Hidden Markov Models** on pages 175-176, 6.3.2.3 **Weight Degeneracy, Rejuvenation, and Effective Sample Size**, and Example 6.3 **Slash Distribution** pages 165-166 of the Givens and Hoeting **Computational Statistics** textbook. [Errata Warning: the right side equation 6.33 on page 175 in section 6.3.2.4 **Sequential Importance Sampling for Hidden Markov Models** has the typo $f_t$ which should be $f_{t-1}$ and the left side of the equation is wrong and should either be $f_{t}(x_{1:t},y_t|y_{1:(t-1)})$, or the normalizing constant $p(y_t|y_{1:(t-1)})$ could be added as a denomenator on the right hand side, or the equation could have alternatively been specified in terms of joint distributions rather than conditional distributions as $f_{t}(x_{1:t},y_{1:t}) = f_{t-1}(x_{1:(t-1)},y_{1:(t-1)})p_x(x_t|x_{t-1})p_y(y_t|x_t)$, e.g., as is done [here](https://www.almoststochastic.com/2013/08/sequential-importance-sampling.html)]*.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not edit this cell\n",
    "T,noise_to_signal = 100,5\n",
    "x,y = np.zeros(T+1),np.zeros(T+1)\n",
    "np.random.seed(9)\n",
    "x[0], y[0] = 0, x[0] + stats.norm.rvs(scale=noise_to_signal, size=1)\n",
    "for t in range(1,T+1):\n",
    "    x[t] = x[t-1] + stats.norm.rvs(size=1)\n",
    "    y[t] = x[t] + stats.norm(scale=noise_to_signal).rvs(size=1)\n",
    "plt.plot(x, label='Signal')\n",
    "plt.plot(y, label='Signal+Noise')\n",
    "plt.title(\"How well can the original signal be detected?\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "# T is the number of time points and we have one time series observation comprised of T time point observations\n",
    "# n above is the number of particles, i.e., importance sampling proposals, each a time series of length T \n",
    "# (There's actually T+1 points because X0=tildeX0=0)\n",
    "\n",
    "tilde_x, w_star, effective_sample_size = np.zeros((n,T+1)), np.ones((n,T+1)), np.ones(T+1)*n\n",
    "# tilde_x is the importance sampling proposals (i.e., stroed across rows)\n",
    "# w_star  is the unnormalized importance sampling proposal weights accumulated up to time t \n",
    "#         for each of the n proposals (i.e., stored down the columns)\n",
    "# effective_sample_size is what it sounds like and is calculated from SIS weights as in the code below\n",
    "\n",
    "# proposal distribution sampling and evaluation\n",
    "# https://en.wikipedia.org/wiki/Slash_distribution\n",
    "slash_pdf = lambda x: (1-np.exp(-x**2/2))/(x**2*np.sqrt(2*np.pi))\n",
    "slash_rvs = lambda n: stats.norm.rvs(size=n)/stats.uniform.rvs(size=n)\n",
    "\n",
    "# particle weights: COMPLETE THIS FUNCTION! When this fucntion works all the remaining code will work.\n",
    "def calculate_SIS_weights(y, tilde_x_t, tilde_x_t_minus_1, w_star_t_minus_1, proposal_pdf):\n",
    "    w_star_t = np.zeros(w_star_t_minus_1.shape)\n",
    "    # <complete>\n",
    "    return w_star_t\n",
    "\n",
    "# normalizing weights for resampling has a known problem which is avoided by this function\n",
    "# https://github.com/numpy/numpy/issues/8317\n",
    "def normalize(w):\n",
    "    pcond = True\n",
    "    while pcond:\n",
    "        w[w==min(w[:-1][w[:-1]>0])] = 0\n",
    "        # https://github.com/scipy/scipy/blob/v1.8.0/scipy/stats/_multivariate.py\n",
    "        # does automatically does this \"correction\" and subsequent `pcond` checks below\n",
    "        w[..., -1] = 1. - w[..., :-1].sum()\n",
    "        # annoyingly, this reassignment of `w[-1]` in `scipy.stats._multivariate.py` \n",
    "        # can cause proper `w` to break, e.g., producing a negative `w[-1]` or a `w.sum()`>1.\n",
    "        # to correct this error which `scipy.stats._multivariate.py` may introduce \n",
    "        # this code iteratively simplifies `w` via `w[w==min(w[:-1][w[:-1]>0])] = 0` above\n",
    "        # and the normalization below until the `pcond` checks in `_multivariate.py` will pass\n",
    "        w = w/np.sort(w)[::-1].sum()\n",
    "        # \"true for bad p\"\n",
    "        pcond = np.any(w < 0, axis=-1)\n",
    "        pcond |= np.any(w > 1, axis=-1)\n",
    "        pcond |= w[:-1].sum()>1\n",
    "    return w        \n",
    "\n",
    "# This is a helper function for particle filter rejuvenation\n",
    "# i.e., bootstrapping SIS according to the particle weights\n",
    "# i.e., converting SIS to SIR (i.e., SIS with resampling) \n",
    "def bootstrap_indices(ireps):\n",
    "    return np.concatenate([r*[i] for i,r in enumerate(ireps)]).flatten().astype(int)\n",
    "    \n",
    "def SIS_wResampling(y, tilde_x, w_star, slash_rvs, slash_pdf, n, T, R):\n",
    "    \n",
    "    # SIS extension loop\n",
    "    for t in range(1, T+1):\n",
    "\n",
    "        # extension of SIS proposal from `tilde_x[:,t-1]` to tilde_x[:,t]\n",
    "        tilde_x[:,t] = tilde_x[:,t-1] + proposal_rvs(n)\n",
    "        # SIS (cumulative) weights: YOU SHOULD HAVE COMPLETED THIS FUNCTION ABOVE!\n",
    "        w_star[:,t] = calculate_SIS_weights(y[t], tilde_x[:,t].copy(), tilde_x[:,t-1].copy(), \n",
    "                                            w_star[:,t-1].copy(), proposal_pdf = proposal_pdf)\n",
    "        # normalizing SIS weights for subsequent bootstrapping, i.e., particle filter rejuvenation\n",
    "        w_star_normalized = w_star[:,t].copy()\n",
    "        w_star_normalized = w_star_normalized/w_star_normalized.sum() # this \"should\" be sufficient\n",
    "        # but because of the issue noted above in `normalize` additional numeric correction is needed\n",
    "        w_star_normalized = normalize(w_star_normalized)  \n",
    "        effective_sample_size[t] = 1/(w_star_normalized**2).sum()\n",
    "        # sequence weights decay over time as a bad proposals at time t erode overal proposal quality\n",
    "        # the weights can be rejuvenated, however, by using a bootstrapping step in the partical filter\n",
    "        if (effective_sample_size[t]<R) or (t==T-1):\n",
    "            # bootstrap partical filter rejuvenation according to the current (normalized) SIS weights\n",
    "            bs_samp = stats.multinomial(n, p=w_star_normalized).rvs(size=1)[0]\n",
    "            tilde_x[:,:(t+1)] = tilde_x[bootstrap_indices(bs_samp),:(t+1)]\n",
    "            w_star[:,t] = 1 # because the weighted importance samples approximate the true\n",
    "            # distribuition, thus they are taken is iid samples from the true distribution\n",
    "    return tilde_x, effective_sample_size\n",
    "\n",
    "np.random.seed(20)            \n",
    "tilde_x, effective_sample_size = SIS_wResampling(y.copy(), tilde_x.copy(), w_star.copy(),\n",
    "                                                 proposal_rvs, proposal_pdf, n, T, n/10)\n",
    "            \n",
    "def plotit():\n",
    "    fig,ax = plt.subplots(2,1, figsize=(15,10))\n",
    "    for i in range(n):\n",
    "        ax[0].plot(tilde_x[i,:], color='gray')\n",
    "    ax[0].plot(x, 'k:', label='latent HMM')\n",
    "    ax[0].plot(y, 'r-', label='Observation')\n",
    "    ax[0].plot(tilde_x.mean(axis=0), 'w-', label='SIS HMM estimate')\n",
    "    ax[0].legend(facecolor='gray', framealpha=.5)\n",
    "    ax[1].plot(effective_sample_size)\n",
    "    ax[1].set_title(\"Effective Sample Size\");\n",
    "plotit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89743c",
   "metadata": {},
   "source": [
    "## Hints:\n",
    "\n",
    "- The questions below are designed to help you complete the necessary code.\n",
    "    - Only update the `calculate_SIS_weights` method.  \n",
    "    - Nothing else in the code should be changed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a95297",
   "metadata": {},
   "source": [
    "### Problem 3 question 0-11 (2 points, 1/6 point each)\n",
    "\n",
    "0. What kind of methodology is implemented by the `SIS_wResampling` function?\n",
    "\n",
    "- (A) Bayesian MCMC analysis\n",
    "- (B) Optimization\n",
    "- (C) Particle filtering\n",
    "- (D) Rejection sampling\n",
    "\n",
    "1. Which of the following does `stats.norm(loc=tilde_x_t, scale=noise_to_signal).pdf(y)` represent?\n",
    "\n",
    "- (A) $p_{Y_t|X_{t}}( y_t | \\tilde x_t)$\n",
    "- (B) $p_{X_t|X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})$\n",
    "- (C) $\\tilde p_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})$\n",
    "- (D) None of the above\n",
    "\n",
    "2. Which of the following can `stats.norm(loc=tilde_x_t, scale=noise_to_signal).pdf(y)` represent?\n",
    "\n",
    "- (A) $p_{Y_1 | X_1}(y_1 | \\tilde x_1)$\n",
    "- (B) $p_{X_1}(\\tilde x_1)$\n",
    "- (C) $\\tilde p_{\\tilde X_1}(\\tilde x_1)$\n",
    "- (D) None of the above\n",
    "\n",
    "4. Which of the following does `stats.norm(loc=tilde_x_t_minus_1, scale=1).pdf(tilde_x_t)` represent?\n",
    "\n",
    "- (A) $p_{Y_t|X_{t}}( y_t | \\tilde x_t)$\n",
    "- (B) $p_{X_t|X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})$\n",
    "- (C) $\\tilde p_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})$\n",
    "- (D) None of the above\n",
    "\n",
    "\n",
    "4. Which of the following can `stats.norm(loc=tilde_x_t_minus_1, scale=1).pdf(tilde_x_t)` represent?\n",
    "\n",
    "- (A) $p_{Y_1 | X_1}(y_1 | \\tilde x_1)$\n",
    "- (B) $p_{X_1}(\\tilde x_1)$\n",
    "- (C) $\\tilde p_{\\tilde X_1}(\\tilde x_1)$\n",
    "- (D) None of the above\n",
    "\n",
    "5. Which of the following does `proposal_pdf(tilde_x_t-tilde_x_t_minus_1)` represent?\n",
    "\n",
    "- (A) $p_{Y_t|X_{t}}( y_t | \\tilde x_t)$\n",
    "- (B) $p_{X_t|X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})$\n",
    "- (C) $\\tilde p_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})$\n",
    "- (D) None of the above\n",
    "\n",
    "6. Which of the following can `proposal_pdf(tilde_x_t-tilde_x_t_minus_1)` represent?\n",
    "\n",
    "- (A) $p_{Y_1 | X_1}(y_1 | \\tilde x_1)$\n",
    "- (B) $p_{X_1}(\\tilde x_1)$\n",
    "- (C) $\\tilde p_{\\tilde X_1}(\\tilde x_1)$\n",
    "- (D) None of the above\n",
    "\n",
    "7. What is $\\frac{p_{X_{1:(t-1)},Y_{1:(t-1)}}(\\tilde x_{1:(t-1)}, y_{1:(t-1)})}{ \\tilde p_{\\tilde X_{1:(t-1)}}(\\tilde x_{1:(t-1)})  }$ equal to?\n",
    "\n",
    "- (A) `w_star_t`\n",
    "- (B) `w_star_t_minus_1`\n",
    "- (C) `w_star_t * w_star_t_minus_1`\n",
    "- (D) None of the above\n",
    "\n",
    "\n",
    "8. Why doesn't the time series match the observed data?\n",
    "\n",
    "- (A) The model assumes noisy data and estimates the latent trend in spite of the noise\n",
    "- (B) More data is required since this is just a single (multivariate time series) observation \n",
    "- (C) The effective sample size is too low and the particles are thus not sufficiently diverse\n",
    "- (D) The model just does not seem to work very well as currently specified\n",
    "\n",
    "9. Why is the predicted trend line smoother toward the end of the time series?\n",
    "\n",
    "- (A) Rejuvenating effective sample size by bootstrapping filters out early sequence diversity \n",
    "- (B) There are more proposal sequences towards then end of the time series versus the beginning\n",
    "- (C) The data is more volatile towards the beginning of the time series versus versus the end\n",
    "- (D) It's not really less smooth and just looks that way due to run to run variation\n",
    "\n",
    "10. If the diversity of the sequences sequences resampled by bootstrapping is reduced due to sample multiplicity from bootstrap sampling, why does the effective sample size still end to be close to the original number of specified particles `n`?\n",
    "\n",
    "- (A) The effective samples size calculation is larger for more homogenous weights\n",
    "- (B) The weights are rejuvenated to 1 through the bootstrap approximation to the true distribution\n",
    "- (C) Both of the above\n",
    "- (D) None of the above\n",
    "\n",
    "11. What is the true cumulative MSE of the bootstrap particle filter estimator?\n",
    "\n",
    "$$\\frac{\\sum_{t=1}^T\\left(\\frac{\\sum_{j=1}^n \\tilde x_{jt}}{n} - x_t\\right)^2}{n}$$ \n",
    "\n",
    "*Note: do not include $t=0$ in your calculation as that's just the inititalization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abe5d5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# p3q0-q11: 1/6 point each [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" based on the choices above]\n",
    "p3q0 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q1 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q2 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q3 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q4 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q5 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q6 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q7 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q8 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q9 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p3q10 = \"\"#<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "# p2q11: [format: `float`]\n",
    "p3q11 = #\n",
    "# This cell will produce a runtime error until the `p1q11` variable is assigned a value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fb80f",
   "metadata": {},
   "source": [
    "### Problem 3 question 12  (3 points)\n",
    "\n",
    "Your code will be checked for accuracy.\n",
    "- No variable assignments are required for this question, but your function `SIS_wResampling` must work when called.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5e07e",
   "metadata": {},
   "source": [
    "# Problem 4 (5 points)\n",
    "\n",
    "For a given set of predefined *bijections* and model fitting scheme (specified below), detemine how many layers (i.e., transformations) the normalizing flow requires in order to morph a standard bivariate (isotropic) Gaussian distribution distribution into a reasonable approximation of Canada's national distribution, i.e., the [\"Uniform Beaver\" distribution](https://www.canada.ca/en/canadian-heritage/services/official-symbols-canada.html#a1).\n",
    "\n",
    "| | | | \n",
    "|-|-|-|\n",
    "|![](https://miro.medium.com/max/1672/1*ATCCfXqMBCn5ZJztzuHjmA.png)|$\\Huge \\longrightarrow$|![](https://www.clipartkey.com/mpngs/m/12-129172_clipart-beaver-silhouette.png)|\n",
    "\n",
    "*This problem draws inspiration from [this TUM project](https://github.com/LukasRinder/normalizing-flows); however, as that project has depreciated with respect to the updated versions of TensorFlow, the additional references indicated in the code comments were indispensable for creating the up to date version presented here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902593ba",
   "metadata": {},
   "source": [
    "## Problem 4 question 0 (5 points)\n",
    "\n",
    "0. Edit the line `transformation_count = 5 # number of bijections in the flow` in the code below \n",
    "to a number that is sufficient to transform a standard bivariate (isotropic) Gaussian into a uniform distribution resembling the beaver above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226987e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 5 points [format: `int`]\n",
    "p4q0 = 0 # replace with the `transformation_count` you used below to address the prompt above\n",
    "# if your number is too small the \"Uniform Beaver\" distribution will not look enough \n",
    "# like Canada's national distribution as shown in the images above and below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b5852",
   "metadata": {},
   "source": [
    "## Hints:\n",
    "\n",
    "To make yourself competitive in the current world of data scientists, machine learners, deep learners, etc., etc., gain skills through the following kinds of resources:\n",
    "- https://www.tensorflow.org/overview/\n",
    "- https://www.tensorflow.org/probability/overview\n",
    "- or similar\n",
    "- etc., etc.\n",
    "\n",
    "> According to a statistic which is an arbitrary I just made up while writing this sentence, each hour you spend improving skills such as those above now will lead to $10,000 dollars of lifetime earnings throughout the course of your career. Or something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f156e4",
   "metadata": {},
   "source": [
    "### Get the image above with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/138250/how-to-read-the-rgb-value-of-a-given-pixel-in-python\n",
    "from PIL import Image\n",
    "import requests, shutil\n",
    "\n",
    "# https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcShdczYYAzEq-nRv50bdqhn9k_yxp9AATT4o01r60R75O7StnqhGl8xdQ0SPUWX_hll8KY&usqp=CAU\n",
    "# https://webapps.stackexchange.com/questions/33997/is-there-a-direct-download-links-for-files-on-google-drive\n",
    "# https://stackoverflow.com/questions/52063556/add-image-to-github-readme-md-from-google-drive\n",
    "r = requests.get('https://drive.google.com/uc?export=download&id=1blBGrvpRUbHVHXwyftP3q6xEry9ZMoBh', stream=True)\n",
    "r.raw.decode_content = True\n",
    "with open('beaver.png', 'wb') as f:\n",
    "    shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "im = Image.open('beaver.png','r')\n",
    "im = np.array(im.getdata()).reshape(im.size, order='F')\n",
    "plt.imshow(im.T==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5a670",
   "metadata": {},
   "source": [
    "### Transform this image into data with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,ys = [],[]\n",
    "for x in range(im.shape[0]):\n",
    "    for y in range(im.shape[1]):\n",
    "        if im[x,y]==1:\n",
    "            xs = xs+[x]\n",
    "            ys = ys+[im.shape[1]-y]\n",
    "\n",
    "ys = np.array(ys)\n",
    "ys = ys - ys.min()\n",
    "ys = ys/ys.max()\n",
    "xs = np.array(xs)/np.max(xs)\n",
    "\n",
    "s = 1000\n",
    "np.random.seed(12)\n",
    "kp = np.random.choice(list(range(xs.shape[0])), s)\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(xs[kp], ys[kp], 'k.')\n",
    "data = np.column_stack([xs, ys]).astype(np.float32)\n",
    "dim = data.shape[-1]\n",
    "n = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224a210",
   "metadata": {},
   "source": [
    "- Experiment with this code below to determine the approximate number of bijections (i.e., transformations) required for the *normalizing flow* (under the bijections as defined) to successfully transform samples from a bivariate normal distribution into samples reasonbly resembling the beaver image (under the provided model fitting scheme). \n",
    "    - I.e., the only line that you need to change is `transformation_count = 5 # number of bijections in the flow`\n",
    "\n",
    "\n",
    "- *Do not alter the defined `shift_and_log_scale_fn` bijection, e.g., do not change `nn_layer_count, nn_layer_variable_count = 2, 128`.  Certainly this specification is in general an open choice, and the usual approach would be to explore alternative specifications for potential improvements; but, for the purposes of this problem, determine the number of bijections (i.e., transformations) required to sufficiently complete the transformation **under the given bijection specification**.*\n",
    "\n",
    "\n",
    "- *In the same manner, do not alter the model fitting scheme specified in `RealNVP_NF.compile` and `RealNVP_NF.fit`, e.g., do not change `optimizer=tf.optimizers.Adam(learning_rate=0.0001)` or `epochs=50, batch_size=1028`. Naturally, optimizer tuning is in general a very important hyperparameter of model fitting and is usually pursued; however, again, for the purposes of this problem, determine the number of bijections (i.e., transformations) required to sufficiently complete the transformation **under the provided model fitting scheme**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265719ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adopted from the outdated https://github.com/LukasRinder/normalizing-flows and\n",
    "# https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/RealNVP\n",
    "import tensorflow as tf\n",
    "tfk = tf.keras\n",
    "tfkl = tfk.layers\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpl = tfp.layers\n",
    "\n",
    "p_Z = tfd.Normal(loc=0.0, scale=1.0) \n",
    "bijectors = []\n",
    "transformation_count = 5 # number of bijections in the flow\n",
    "for i in range(0, transformation_count):\n",
    "    if i != 0:\n",
    "        # alternate which variable gets the identity transformation\n",
    "        bijectors.append(tfb.Permute(permutation=[1, 0]))\n",
    "\n",
    "    # Below, in `bijectors.append(...`\n",
    "    # `shift_and_log_scale_fn = tfb.real_nvp_default_template(hidden_layers=hidden_shape)`\n",
    "    # https://github.com/tensorflow/probability/blob/v0.13.0/tensorflow_probability/python/bijectors/real_nvp.py#L335-L413\n",
    "    # is not keras/TF2.0 compatible as it is TF1.0 based, so not usable with keras\n",
    "    # https://github.com/tensorflow/probability/issues/355\n",
    "    # However, rather than recreating the entire Real NVP object\n",
    "    # as is done here https://keras.io/examples/generative/real_nvp/\n",
    "    # we instead create a keras/TF2.0 compatible variant of `real_nvp_default_template`\n",
    "    half = int(dim/2)                      \n",
    "    input = tfkl.Input(shape=(half), dtype=tf.float32)\n",
    "    nn_layer_count, nn_layer_variable_count = 2, 128\n",
    "    nn_shift = tfk.Sequential(\n",
    "      [tfkl.Dense(nn_layer_variable_count, activation='relu')\n",
    "       for i in range(nn_layer_count)] + [tfkl.Dense(half)])\n",
    "    nn_log_scale = tfk.Sequential(\n",
    "      [tfkl.Dense(nn_layer_variable_count, activation='relu')\n",
    "       for i in range(nn_layer_count)] + [tfkl.Dense(half)])\n",
    "    shift_and_log_scale_fn = tfk.Model(input, [nn_shift(input), nn_log_scale(input)])  \n",
    "    # `tfb.RealNVP` manages data spliting, and passes the part being used\n",
    "    # - `input` thus gets `half` not `dim` (for 0:dim/2 and dim/2:dim split)\n",
    "    # `shift_and_log_scale_fn` may any desired neural network \n",
    "    # - we use independent shift and log_scale neural networks \n",
    "    # - with `2` layers of `128` hidden variables with a `half` sized output layer\n",
    "    # - respectively, these unconstrained outputs define `shift` and `log_scale` \n",
    "    # - yielding transformations of the `half` input variables\n",
    "    # - which `tfb.RealNVP` then combinesback with the untransformed variables\n",
    "    # The list output of `shift_log_scale_tuple` is what `tfb.RealNVP` expects\n",
    "    # `shift_and_log_scale_fn` thus replaces `real_nvp_default_template`\n",
    "    bijectors.append(\n",
    "    tfb.RealNVP(fraction_masked=0.5, shift_and_log_scale_fn=shift_and_log_scale_fn))\n",
    "\n",
    "# as `shift_and_log_scale_fn` is defined, must use `p_X.sample(sample_shape=n)` \n",
    "p_X = tfd.TransformedDistribution(\n",
    "        distribution=tfd.Sample(p_Z, sample_shape=dim),\n",
    "        bijector=tfb.Chain(bijectors=bijectors, name='RealNVP_sequence'))\n",
    "\n",
    "x_ = tfkl.Input(shape=(data.shape[-1],), dtype=tf.float32)\n",
    "log_prob_ = p_X.log_prob(x_)\n",
    "RealNVP_NF = tfk.Model(x_, log_prob_)\n",
    "RealNVP_NF.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), \n",
    "                   loss=lambda x, log_prob: -log_prob) # our target log(p(x)=1) is 0\n",
    "tf.random.set_seed(13)                   \n",
    "RealNVP_NF.fit(x=data-0.5, y=np.zeros((n, 0), dtype=np.float32),\n",
    "               epochs=50, batch_size=1028, shuffle=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c584130",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_n = 50\n",
    "grid = np.linspace(0, 1, grid_n)-0.5\n",
    "grid_x, grid_y = np.meshgrid(grid, grid)\n",
    "p_x = RealNVP_NF(np.column_stack([grid_x.flatten(), grid_y.flatten()]))\n",
    "p_x = p_x.numpy().reshape(grid_n, grid_n)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,4))\n",
    "ax[0].contour(grid_x, grid_y, p_x)\n",
    "ax[0].plot(data[kp,0]-0.5, data[kp,1]-0.5, 'k.')\n",
    "\n",
    "x1x2 = p_X.sample(sample_shape=2500)\n",
    "ax[1].plot(x1x2[:,0], x1x2[:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38781a6c",
   "metadata": {},
   "source": [
    "# You're done with all the coding work in the course... Congratulations!\n",
    "\n",
    "## Now you just have a final exam worth 60 points (20% of course grade)... Happy Studying!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
